{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Apache Spark](media/00.apache-spark.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - INTRODUCCIÓN A APACHE SPARK\n",
    "---\n",
    "\n",
    "Apache Spark se puede definir cómo una plataforma de computación cluster rápida que e ha pensado para ejecutar programas paralelos en cluster con decenas y centenares de máquinas.\n",
    "\n",
    "Lo que hace es extender el modelo MapReduce desarrollado inicialmente por google y llevado al mundo open source en apache hadoop.\n",
    "\n",
    "El modelo map reduce inicial solo soportaba un tipo de computación, denominada computación en batch.\n",
    "* Básicamente lo que hace es que tenemos un montón de datos almacenados en los discos duros de los nodos de nuestros cluster y ejecutamos un procesamiento.\n",
    "\n",
    "\n",
    "## Plataforma de computación cluster rápida\n",
    "\n",
    "Extiende modelo MapReduce soportando de manera eficiente otros tipos de computación:\n",
    "* Queries interactivas\n",
    "* Procesado streaming\n",
    "* Soporta computaciones en memoria\n",
    "* Mejora a MapReduce para aplicaciones complejas (10-20x más rápido)\n",
    "* Reduce el uso del disco que en el caso de hadoop es bastante elevado.\n",
    "\n",
    "\n",
    "## Propósito general\n",
    "\n",
    "* Modos de funcionamiento batch, interactivo o streaming\n",
    "* Reduce el número de herramientas a emplear y mantener\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historia\n",
    "\n",
    "Inicia en el **2009** en el UC Berkeley RAD Lab (AMPLab)\n",
    "* Motivado por la ineficiencia de MapReduce para trabajos iterativos e interactivos\n",
    "\n",
    "Mayores contribuidores: \n",
    "* Databricks,\n",
    "* Yahoo! e Intel\n",
    "\n",
    "Declarado open source en **marzo del 2010**\n",
    "\n",
    "Transferido a la **Apache Software Foundation en junio de 2013**, TLP en **febrero de 2014**\n",
    "\n",
    "Uno de los proyectos Big Data **más activos**\n",
    "\n",
    "Versión 1.0 lanzada en **mayo de 2014**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Características de Spark\n",
    "\n",
    "Soporta gran variedad de workloads: batch, queries interactivas, streaming, machine learning, procesado de grafos\n",
    "\n",
    "APIs en **Scala, Java, Python, SQL y R**\n",
    "\n",
    "Shells interactivos en **Scala y Python**\n",
    "\n",
    "Se integra con otras soluciones BigData: **HDFS, Cassandra, etc**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La Pila Spark\n",
    "\n",
    "![La pila Spark](media/01.pila_spark.png)\n",
    "\n",
    "(Fuente: H. Karau, A. Konwinski, P. Wendell, M. Zaharia, “Learning Spark”, O’Reilly, 2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptos clave\n",
    "\n",
    "![Conceptos clave](media/02.key_concepts.png)\n",
    "\n",
    "(Fuente: H. Karau, A. Konwinski, P. Wendell, M. Zaharia, “Learning Spark”, O’Reilly, 2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver\n",
    "\n",
    "Crea un SparkContext\n",
    "\n",
    "Convierte el programa de usuario en tareas:\n",
    "* DAG de operaciones lógico -> plan de ejecución físico\n",
    "\n",
    "Planifica las tareas en los ejecutores\n",
    "\n",
    "### Spark Context\n",
    "\n",
    "El SparkContext realiza la conexión con el cluster\n",
    "* Permite construir RDDs a partir de ficheros, listas u otros objetos\n",
    "\n",
    "En el notebook (o el shell de Spark), se define automáticamente (variable `sc`)\n",
    "\n",
    "Creación en un script Python:\n",
    "\n",
    "```py\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master=\"local\", appName=\"Mi app\")\n",
    "```\n",
    "\n",
    "Creación en un programa Scala:\n",
    "\n",
    "```scala\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.SparkConf\n",
    "val conf = new SparkConf().setMaster(\"local\").setAppName(\"My App\")\n",
    "val sc = new SparkContext(conf)\n",
    "```\n",
    "\n",
    "### Executors\n",
    "\n",
    "Ejecutan las tareas individuales y devuelven los resultados al Driver\n",
    "\n",
    "Proporcionan almacenamiento en memoria para los datos de las tareas\n",
    "\n",
    "### Cluster Manager\n",
    "\n",
    "Componente *enchufable* en Spark\n",
    "\n",
    "YARN, Mesos o Spark Standalone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Instalación de pySpark y configuración de Spark Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede trabajar con la API de Python o de Scala, para este notebook todo lo trabajaremos con Python.\n",
    "\n",
    "* API PySpark: http://spark.apache.org/docs/latest/api/python/index.html\n",
    "* API Scala: https://spark.apache.org/docs/latest/api/scala/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /home/xergioalex/Envs/tensorflow/lib/python3.5/site-packages\n",
      "Requirement already satisfied: py4j==0.10.7 in /home/xergioalex/Envs/tensorflow/lib/python3.5/site-packages (from pyspark)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create apache spark context\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master=\"local\", appName=\"Mi app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop apache spark context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RDDs](media/03.rdd_partition.png)\n",
    "\n",
    "# 03 - RDD: RESILENT DISTRIBUTED DATASETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colección inmutable y distribuida de elementos que pueden manipularse en paralelo\n",
    "\n",
    "Un programa Spark opera sobre RDDs:\n",
    "\n",
    "Spark automáticamente distribuye los datos y paraleliza las operaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de RDDs\n",
    "\n",
    "Se pueden crear de dos formas:\n",
    "1. Paralelizando una colección en el programa driver\n",
    "2. Leyendo datos de un fichero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd1:  [[1, 2, 3]]\n",
      "rdd2:  [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]]\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo en PySpark\n",
    "rdd1 = sc.parallelize([1,2,3])\n",
    "print(\"rdd1: \", rdd1.glom().collect())\n",
    "\n",
    "import numpy as np\n",
    "rdd2 = sc.parallelize(np.array(range(100)))\n",
    "print(\"rdd2: \", rdd2.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo en PySpark (fichero ../datos/quijote.txt)\n",
    "quijote = sc.textFile(\"../datos/quijote.txt\")\n",
    "print(quijote.take(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
