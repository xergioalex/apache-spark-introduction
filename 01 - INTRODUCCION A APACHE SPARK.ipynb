{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Apache Spark](media/00.apache-spark.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - INTRODUCCIÓN A APACHE SPARK\n",
    "---\n",
    "\n",
    "Apache Spark se puede definir cómo una plataforma de computación cluster rápida que e ha pensado para ejecutar programas paralelos en cluster con decenas y centenares de máquinas.\n",
    "\n",
    "Lo que hace es extender el modelo MapReduce desarrollado inicialmente por google y llevado al mundo open source en apache hadoop.\n",
    "\n",
    "El modelo map reduce inicial solo soportaba un tipo de computación, denominada computación en batch.\n",
    "* Básicamente lo que hace es que tenemos un montón de datos almacenados en los discos duros de los nodos de nuestros cluster y ejecutamos un procesamiento.\n",
    "\n",
    "\n",
    "## Plataforma de computación cluster rápida\n",
    "\n",
    "Extiende modelo MapReduce soportando de manera eficiente otros tipos de computación:\n",
    "* Queries interactivas\n",
    "* Procesado streaming\n",
    "* Soporta computaciones en memoria\n",
    "* Mejora a MapReduce para aplicaciones complejas (10-20x más rápido)\n",
    "* Reduce el uso del disco que en el caso de hadoop es bastante elevado.\n",
    "\n",
    "\n",
    "## Propósito general\n",
    "\n",
    "* Modos de funcionamiento batch, interactivo o streaming\n",
    "* Reduce el número de herramientas a emplear y mantener\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historia\n",
    "\n",
    "Inicia en el **2009** en el UC Berkeley RAD Lab (AMPLab)\n",
    "* Motivado por la ineficiencia de MapReduce para trabajos iterativos e interactivos\n",
    "\n",
    "Mayores contribuidores: \n",
    "* Databricks,\n",
    "* Yahoo! e Intel\n",
    "\n",
    "Declarado open source en **marzo del 2010**\n",
    "\n",
    "Transferido a la **Apache Software Foundation en junio de 2013**, TLP en **febrero de 2014**\n",
    "\n",
    "Uno de los proyectos Big Data **más activos**\n",
    "\n",
    "Versión 1.0 lanzada en **mayo de 2014**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Características de Spark\n",
    "\n",
    "Soporta gran variedad de workloads: batch, queries interactivas, streaming, machine learning, procesado de grafos\n",
    "\n",
    "APIs en **Scala, Java, Python, SQL y R**\n",
    "\n",
    "Shells interactivos en **Scala y Python**\n",
    "\n",
    "Se integra con otras soluciones BigData: **HDFS, Cassandra, etc**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La Pila Spark\n",
    "\n",
    "![La pila Spark](media/01.pila_spark.png)\n",
    "\n",
    "(Fuente: H. Karau, A. Konwinski, P. Wendell, M. Zaharia, “Learning Spark”, O’Reilly, 2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptos clave\n",
    "\n",
    "![Conceptos clave](media/02.key_concepts.png)\n",
    "\n",
    "(Fuente: H. Karau, A. Konwinski, P. Wendell, M. Zaharia, “Learning Spark”, O’Reilly, 2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver\n",
    "\n",
    "Crea un SparkContext\n",
    "\n",
    "Convierte el programa de usuario en tareas:\n",
    "* DAG de operaciones lógico -> plan de ejecución físico\n",
    "\n",
    "Planifica las tareas en los ejecutores\n",
    "\n",
    "### Spark Context\n",
    "\n",
    "El SparkContext realiza la conexión con el cluster\n",
    "* Permite construir RDDs a partir de ficheros, listas u otros objetos\n",
    "\n",
    "En el notebook (o el shell de Spark), se define automáticamente (variable `sc`)\n",
    "\n",
    "Creación en un script Python:\n",
    "\n",
    "```py\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master=\"local\", appName=\"Mi app\")\n",
    "```\n",
    "\n",
    "Creación en un programa Scala:\n",
    "\n",
    "```scala\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.SparkConf\n",
    "val conf = new SparkConf().setMaster(\"local\").setAppName(\"My App\")\n",
    "val sc = new SparkContext(conf)\n",
    "```\n",
    "\n",
    "### Executors\n",
    "\n",
    "Ejecutan las tareas individuales y devuelven los resultados al Driver\n",
    "\n",
    "Proporcionan almacenamiento en memoria para los datos de las tareas\n",
    "\n",
    "### Cluster Manager\n",
    "\n",
    "Componente *enchufable* en Spark\n",
    "\n",
    "YARN, Mesos o Spark Standalone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalación de pySpark y configuración de Spark Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede trabajar con la API de Python o de Scala, para este notebook todo lo trabajaremos con Python.\n",
    "\n",
    "* API PySpark: http://spark.apache.org/docs/latest/api/python/index.html\n",
    "* API Scala: https://spark.apache.org/docs/latest/api/scala/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /home/xergioalex/Envs/tensorflow/lib/python3.5/site-packages\n",
      "Requirement already satisfied: py4j==0.10.7 in /home/xergioalex/Envs/tensorflow/lib/python3.5/site-packages (from pyspark)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create apache spark context\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master=\"local\", appName=\"Mi app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop apache spark context\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
