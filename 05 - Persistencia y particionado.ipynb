{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RDD key pair](media/06.rdd_persistence.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 05- Persistencia y particionado\n",
    "--------------\n",
    "\n",
    "### Persistencia\n",
    "\n",
    "Problema al usar un RDD varias veces:\n",
    "\n",
    "-   Spark recomputa el RDD y sus dependencias cada vez que se ejecuta una acción\n",
    "-   Muy costoso (especialmente en problemas iterativos)\n",
    "\n",
    "Solución\n",
    "\n",
    "-   Conservar el RDD en memoria y/o disco\n",
    "-   Métodos `cache()` o `persist()`\n",
    "\n",
    "#### Niveles de persistencia (definidos en [`pyspark.StorageLevel`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.StorageLevel))\n",
    " Nivel                | Espacio  | CPU     | Memoria/Disco   | Descripción\n",
    " :------------------: | :------: | :-----: | :-------------: | ------------------\n",
    " MEMORY_ONLY          |   Alto   |   Bajo  |     Memoria     | Guarda el RDD como un objeto Java no serializado en la JVM. Si el RDD no cabe en memoria, algunas particiones no se *cachearán* y serán recomputadas \"al vuelo\" cada vez que se necesiten. Nivel por defecto en Java y Scala.\n",
    " MEMORY_ONLY_SER      |   Bajo   |   Alto  |     Memoria     | Guarda el RDD como un objeto Java serializado (un *byte array* por partición). Nivel por defecto en Python, usando [`pickle`](http://docs.python.org/2/library/pickle.html).\n",
    " MEMORY_AND_DISK      |   Alto   |   Medio |     Ambos       | Guarda el RDD como un objeto Java no serializado en la JVM. Si el RDD no cabe en memoria, las particiones que no quepan se guardan en disco y se leen del mismo cada vez que se necesiten\n",
    " MEMORY_AND_DISK_SER  |   Bajo   |   Alto  |     Ambos       | Similar a MEMORY_AND_DISK pero usando objetos serializados.\n",
    " DISK_ONLY            |   Bajo   |   Alto  |     Disco       | Guarda las particiones del RDD solo en disco.\n",
    " OFF_HEAP             |   Bajo   |   Alto  |   Memoria       | Guarda el RDD serializado usando memoria *off-heap* (fuera del heap de la JVM) lo que puede reducir el overhead del recolector de basura\n",
    "   \n",
    "\n",
    "\n",
    "    \n",
    "#### Nivel de persistencia\n",
    "\n",
    "-   En Scala y Java, el nivel por defecto es MEMORY\\_ONLY\n",
    "\n",
    "-   En Python, los datos siempre se serializan (por defecto como objetos *pickled*)\n",
    "\n",
    "    -   Los niveles MEMORY_ONLY, MEMORY_AND_DISK son equivalentes a MEMORY_ONLY_SER, MEMORY_AND_DISK_SER\n",
    "    - Es posible especificar serialización [`marshal`](https://docs.python.org/2/library/marshal.html#module-marshal) al crear el SparkContext\n",
    "    \n",
    "```python\n",
    "sc = SparkContext(master=\"local\", appName=\"Mi app\", serializer=pyspark.MarshalSerializer())\n",
    "```\n",
    "    \n",
    "#### Recuperación de fallos\n",
    "\n",
    "-   Si falla un nodo con datos almacenados, el RDD se recomputa\n",
    "\n",
    "    -   Añadiendo `_2` al nivel de persistencia, se guardan 2 copias del RDD\n",
    "        \n",
    "#### Gestión de la cache\n",
    "\n",
    "-   Algoritmo LRU para gestionar la cache\n",
    "\n",
    "    -   Para niveles *solo memoria*, los RDDs viejos se eliminan y se recalculan\n",
    "    -   Para niveles *memoria y disco*, las particiones que no caben se escriben a disco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /home/xergioalex/Envs/tensorflow/lib/python3.5/site-packages\n",
      "Requirement already satisfied: py4j==0.10.7 in /home/xergioalex/Envs/tensorflow/lib/python3.5/site-packages (from pyspark)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create apache spark context\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master=\"local\", appName=\"Mi app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop apache spark context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Debugging if Cache is active\n",
    "rdd = sc.parallelize(range(1000), 10)\n",
    "\n",
    "print(rdd.is_cached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Nivel de persistencia de rdd: Disk Memory Serialized 2x Replicated\n"
     ]
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "rdd.persist(StorageLevel.MEMORY_AND_DISK_SER_2)\n",
    "\n",
    "print(rdd.is_cached)\n",
    "\n",
    "print(\"Nivel de persistencia de rdd: {0}\".format(rdd.getStorageLevel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "rdd2 = rdd.map(lambda x: x*x)\n",
    "print(rdd2.is_cached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Nivel de persistencia de rdd2: Memory Serialized 1x Replicated\n"
     ]
    }
   ],
   "source": [
    "rdd2.cache() # Nivel por defecto\n",
    "print(rdd2.is_cached)\n",
    "print(\"Nivel de persistencia de rdd2: {0}\".format(rdd2.getStorageLevel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "rdd2.unpersist() # Sacamos rdd2 de la cache\n",
    "print(rdd2.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Particionado\n",
    "\n",
    "El número de particiones es función del tamaño del cluster o el número de bloques del fichero en HDFS\n",
    "\n",
    "-   Es posible ajustarlo al crear u operar sobre un RDD\n",
    "\n",
    "-   El paralelismo de RDDs que derivan de otros depende del de sus RDDs padre\n",
    "\n",
    "-   Dos funciones útiles:\n",
    "\n",
    "    -   `rdd.getNumPartitions()` devuelve el número de particiones del RDD\n",
    "    -   `rdd.glom()` devuelve un nuevo RDD juntando los elementos de cada partición en una lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD pairs = [(1, 1), (2, 2), (3, 3), (4, 4), (2, 2), (4, 4), (1, 1)]\n",
      "Particionado de pairs: [[(1, 1)], [(2, 2), (3, 3)], [(4, 4), (2, 2)], [(4, 4), (1, 1)]]\n",
      "Número de particiones de pairs = 4\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4, 2, 4, 1], 4)\n",
    "pairs = rdd.map(lambda x: (x, x))\n",
    "\n",
    "print(\"RDD pairs = {0}\".format(pairs.collect()))\n",
    "print(\"Particionado de pairs: {0}\".format(pairs.glom().collect()))\n",
    "print(\"Número de particiones de pairs = {0}\".format(pairs.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducción con 4 particiones: [[(4, 8)], [(1, 2)], [(2, 4)], [(3, 3)]]\n"
     ]
    }
   ],
   "source": [
    "# Reducción manteniendo el número de particiones\n",
    "print(\"Reducción con 4 particiones: {0}\".format(pairs.reduceByKey(lambda x, y: x+y).glom().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducción con 2 particiones: [[(2, 4), (4, 8)], [(1, 2), (3, 3)]]\n"
     ]
    }
   ],
   "source": [
    "# Reducción modificando el número de particiones\n",
    "print(\"Reducción con 2 particiones: {0}\".format(pairs.reduceByKey(lambda x, y: x+y, 2).glom().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciones de reparticionado\n",
    "- `repartition(n)` devuelve un nuevo RDD que tiene exactamente `n` particiones\n",
    "- `coalesce(n)` más eficiente que `repartition`, minimiza el movimiento de datos\n",
    "    - Solo permite reducir el número de particiones\n",
    "- `partitionBy(n,[partitionFunc])` Particiona por clave, usando una función de particionado (por defecto, un hash de la clave)\n",
    "    - Solo para RDDs clave/valor\n",
    "    - Asegura que los pares con la misma clave vayan a la misma partición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pairs5 con 5 particiones: [[], [(1, 1)], [(4, 4), (2, 2), (4, 4), (1, 1)], [(2, 2), (3, 3)], []]\n"
     ]
    }
   ],
   "source": [
    "pairs5 = pairs.repartition(5)\n",
    "print(\"pairs5 con {0} particiones: {1}\".format(\n",
    "        pairs5.getNumPartitions(),\n",
    "        pairs5.glom().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pairs2 con 2 particiones: [[(1, 1)], [(4, 4), (2, 2), (4, 4), (1, 1), (2, 2), (3, 3)]]\n"
     ]
    }
   ],
   "source": [
    "pairs2 = pairs5.coalesce(2)\n",
    "print(\"pairs2 con {0} particiones: {1}\".format(\n",
    "        pairs2.getNumPartitions(),\n",
    "        pairs2.glom().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Particionado por clave (3 particiones): [[(3, 3)], [(1, 1), (4, 4), (4, 4), (1, 1)], [(2, 2), (2, 2)]]\n"
     ]
    }
   ],
   "source": [
    "pairs_clave = pairs2.partitionBy(3)\n",
    "print(\"Particionado por clave ({0} particiones): {1}\".format(\n",
    "        pairs_clave.getNumPartitions(),\n",
    "        pairs_clave.glom().collect())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
